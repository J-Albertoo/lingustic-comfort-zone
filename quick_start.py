# quick_start.py - Vers√£o corrigida e melhorada

from src.data_loader import EnronDataLoader
from src.analyzer import LinguisticAnalyzer
from wordcloud import WordCloud
import matplotlib.pyplot as plt
import os

def quick_start_analysis():
    """Script para come√ßar a an√°lise rapidamente"""
    
    # 1. Carregar dados
    print("üöÄ Iniciando Linguistic Comfort Zone Mapper!")
    print("-" * 50)
    
    # Verificar se o arquivo existe
    data_path = 'data/raw/'
    if not os.path.exists(os.path.join(data_path, 'emails.csv')):
        print("‚ùå ERRO: Arquivo 'emails.csv' n√£o encontrado em data/raw/")
        print("üì• Por favor, baixe o dataset do Kaggle primeiro:")
        print("   https://www.kaggle.com/datasets/wcukierski/enron-email-dataset")
        return
    
    loader = EnronDataLoader(data_path)
    
    # Come√ßar com subset pequeno para testar
    df = loader.load_emails_from_csv(limit=5000)
    
    # 2. Preparar dados por pessoa
    person_emails = loader.get_emails_by_person(min_emails=30)
    
    if not person_emails:
        print("‚ùå Nenhuma pessoa encontrada com emails suficientes!")
        return
    
    # 3. Analisar primeira pessoa
    analyzer = LinguisticAnalyzer()
    
    # Pegar primeira pessoa com muitos emails
    first_person = list(person_emails.keys())[0]
    emails = person_emails[first_person]
    
    print(f"\nüìä Analisando padr√µes lingu√≠sticos de: {first_person}")
    print(f"üìß Total de emails: {len(emails)}")
    print("-" * 50)
    
    # 4. Rodar an√°lise
    results = analyzer.analyze_person(emails, first_person)
    
    # 5. Mostrar resultados
    print("\nüéØ TOP 10 V√çCIOS LINGU√çSTICOS:")
    for word, count in results['comfort_words'][:10]:
        print(f"  ‚Ä¢ {word}: {count} vezes")
    
    print("\nüí¨ FRASES FAVORITAS:")
    for phrase, count in results['favorite_phrases'][:5]:
        print(f"  ‚Ä¢ '{phrase}': {count} vezes")
    
    print("\nüìù ESTILO DE ESCRITA:")
    style = results['writing_style']
    print(f"  ‚Ä¢ Tamanho m√©dio de frase: {style['avg_sentence_length']:.1f} palavras")
    print(f"  ‚Ä¢ Facilidade de leitura: {style['reading_ease']:.1f}/100")
    print(f"  ‚Ä¢ Uso de exclama√ß√µes: {style['exclamation_usage']:.2%}")
    
    print("\nüìä DIVERSIDADE VOCABULAR:")
    div = results['vocabulary_diversity']
    print(f"  ‚Ä¢ Palavras √∫nicas: {div['unique_words']:,}")
    print(f"  ‚Ä¢ Diversidade lexical: {div['lexical_diversity']:.2%}")
    
    print("\n‚ú® ASSINATURA LINGU√çSTICA:")
    finger = results['linguistic_fingerprint']
    print(f"  ‚Ä¢ Estilo de sauda√ß√£o: {results['email_patterns']['greeting_style']}")
    print(f"  ‚Ä¢ Palavras de transi√ß√£o favoritas: {list(finger['transition_words'].keys())[:3]}")
    
    # 6. CRIAR VISUALIZA√á√ïES
    create_visualizations(results, first_person)
    
    print("\n‚úÖ An√°lise inicial completa! Verifique as imagens geradas! üìä")
    
    return results, first_person, person_emails

def create_visualizations(results, person_name):
    """Cria visualiza√ß√µes dos resultados"""
    
    # Criar pasta para outputs se n√£o existir
    os.makedirs('outputs', exist_ok=True)
    
    # 1. Word Cloud dos v√≠cios lingu√≠sticos
    print("\nüé® Gerando visualiza√ß√µes...")
    
    words_freq = dict(results['comfort_words'])
    
    # Configurar word cloud com cores profissionais
    wordcloud = WordCloud(
        width=1200, 
        height=600, 
        background_color='white',
        colormap='viridis',
        max_words=50,
        relative_scaling=0.5,
        min_font_size=10
    ).generate_from_frequencies(words_freq)
    
    # Criar figura
    plt.figure(figsize=(12, 6))
    plt.imshow(wordcloud, interpolation='bilinear')
    plt.axis('off')
    plt.title(f'V√≠cios Lingu√≠sticos de {person_name}', fontsize=16, pad=20)
    plt.tight_layout()
    
    # Salvar
    wordcloud_path = 'outputs/wordcloud_vicios.png'
    plt.savefig(wordcloud_path, dpi=300, bbox_inches='tight')
    print(f"  ‚úì Word cloud salva em: {wordcloud_path}")
    
    # 2. Gr√°fico de barras dos top v√≠cios
    plt.figure(figsize=(10, 6))
    
    # Pegar top 15 palavras
    top_words = results['comfort_words'][:15]
    words = [w[0] for w in top_words]
    counts = [w[1] for w in top_words]
    
    # Criar gr√°fico de barras horizontal
    plt.barh(words, counts, color='steelblue')
    plt.xlabel('Frequ√™ncia', fontsize=12)
    plt.title(f'Top 15 Palavras Mais Usadas - {person_name}', fontsize=14, pad=20)
    plt.gca().invert_yaxis()  # Maior no topo
    
    # Adicionar valores nas barras
    for i, (word, count) in enumerate(top_words):
        plt.text(count + 5, i, str(count), va='center')
    
    plt.tight_layout()
    
    # Salvar
    bars_path = 'outputs/top_palavras.png'
    plt.savefig(bars_path, dpi=300, bbox_inches='tight')
    print(f"  ‚úì Gr√°fico de barras salvo em: {bars_path}")
    
    # 3. An√°lise de estilo de escrita (radar chart)
    import numpy as np
    
    fig, ax = plt.subplots(figsize=(8, 8), subplot_kw=dict(projection='polar'))
    
    # Preparar dados para o radar
    categories = ['Facilidade\nLeitura', 'Uso de\nExclama√ß√µes', 'Uso de\nPerguntas', 
                  'Diversidade\nLexical', 'Tamanho\nFrases']
    
    style = results['writing_style']
    div = results['vocabulary_diversity']
    
    # Normalizar valores para escala 0-100
    values = [
        style['reading_ease'],
        style['exclamation_usage'] * 100,
        style['question_usage'] * 100,
        div['lexical_diversity'] * 100,
        min(style['avg_sentence_length'] / 30 * 100, 100)  # Normalizar para max 30 palavras
    ]
    
    # Fechar o radar
    values += values[:1]
    angles = np.linspace(0, 2 * np.pi, len(categories), endpoint=False).tolist()
    angles += angles[:1]
    
    # Plot
    ax.plot(angles, values, 'o-', linewidth=2, color='darkblue')
    ax.fill(angles, values, alpha=0.25, color='skyblue')
    
    # Labels
    ax.set_theta_offset(np.pi / 2)
    ax.set_theta_direction(-1)
    ax.set_xticks(angles[:-1])
    ax.set_xticklabels(categories)
    ax.set_ylim(0, 100)
    ax.set_title(f'Perfil de Estilo de Escrita - {person_name}', y=1.08, fontsize=14)
    
    plt.tight_layout()
    
    # Salvar
    radar_path = 'outputs/estilo_escrita_radar.png'
    plt.savefig(radar_path, dpi=300, bbox_inches='tight')
    print(f"  ‚úì Radar de estilo salvo em: {radar_path}")
    
    plt.close('all')  # Fechar todas as figuras

def analyze_multiple_people(person_emails, analyzer, max_people=5):
    """Analisa m√∫ltiplas pessoas para compara√ß√£o"""
    
    print(f"\nüîç Analisando {min(max_people, len(person_emails))} pessoas para compara√ß√£o...")
    
    all_results = {}
    
    for i, (person, emails) in enumerate(person_emails.items()):
        if i >= max_people:
            break
            
        print(f"  ‚Ä¢ Analisando {person}... ({i+1}/{max_people})")
        results = analyzer.analyze_person(emails, person)
        all_results[person] = results
    
    return all_results

def create_comparison_visualization(all_results):
    """Cria visualiza√ß√£o comparando m√∫ltiplas pessoas"""
    
    import pandas as pd
    
    # Preparar dados para compara√ß√£o
    comparison_data = []
    
    for person, results in all_results.items():
        comparison_data.append({
            'Pessoa': person.split('@')[0],  # Apenas nome antes do @
            'Diversidade Lexical': results['vocabulary_diversity']['lexical_diversity'] * 100,
            'Facilidade Leitura': results['writing_style']['reading_ease'],
            'Emails Analisados': results['total_emails']
        })
    
    df = pd.DataFrame(comparison_data)
    
    # Criar gr√°fico de compara√ß√£o
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))
    
    # Gr√°fico 1: Diversidade Lexical
    df.plot(x='Pessoa', y='Diversidade Lexical', kind='bar', ax=ax1, color='teal')
    ax1.set_title('Diversidade Lexical por Pessoa', fontsize=14)
    ax1.set_ylabel('Diversidade (%)')
    ax1.set_xlabel('')
    ax1.set_xticklabels(df['Pessoa'], rotation=45, ha='right')
    
    # Gr√°fico 2: Facilidade de Leitura
    df.plot(x='Pessoa', y='Facilidade Leitura', kind='bar', ax=ax2, color='coral')
    ax2.set_title('Facilidade de Leitura por Pessoa', fontsize=14)
    ax2.set_ylabel('Score Flesch (0-100)')
    ax2.set_xlabel('')
    ax2.set_xticklabels(df['Pessoa'], rotation=45, ha='right')
    
    plt.suptitle('Compara√ß√£o de Estilos de Escrita - Dataset Enron', fontsize=16)
    plt.tight_layout()
    
    # Salvar
    comparison_path = 'outputs/comparacao_pessoas.png'
    plt.savefig(comparison_path, dpi=300, bbox_inches='tight')
    print(f"\n‚úì Gr√°fico de compara√ß√£o salvo em: {comparison_path}")
    
    plt.close()

if __name__ == "__main__":
    # Rodar an√°lise principal
    result = quick_start_analysis()
    
    if result:  # Se a an√°lise foi bem sucedida
        results, first_person, person_emails = result
        
        # Perguntar se quer analisar mais pessoas
        print("\n" + "="*50)
        print("ü§î Deseja analisar mais pessoas para compara√ß√£o? (s/n)")
        
        # Para automa√ß√£o, vamos assumir 'sim'
        # Em produ√ß√£o, voc√™ usaria: response = input().lower()
        response = 's'  # Automatizado para o exemplo
        
        if response == 's':
            analyzer = LinguisticAnalyzer()
            all_results = analyze_multiple_people(person_emails, analyzer, max_people=5)
            create_comparison_visualization(all_results)
            
            print("\nüéâ An√°lise completa! Verifique a pasta 'outputs' para todas as visualiza√ß√µes!")
            print("\nüìä Pr√≥ximos passos:")
            print("  1. Analise os gr√°ficos gerados")
            print("  2. Identifique padr√µes interessantes")
            print("  3. Prepare insights para o LinkedIn")
            print("  4. Comece a construir o app Streamlit!")